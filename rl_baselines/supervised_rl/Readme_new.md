#  Steps for Generative Replay(Without having access to any envrionement)
Note: Each enviroment on-policy dataset for doing the distillation is generated by a generative model of its own environment.
So we do not need to be albe to access to each environment. 

For more detail about the experiments and results, see: [Report_PRE_bunthet_SAY.pdf](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Report_PRE_bunthet_SAY.pdf) or [Soutenance_PRE.pdf](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Soutenance%20_PRE.pdf)

![alt text](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Img/Experiments.png)

#  EXPERIMENT 1:

Representation of the process in Experiment 1
=============================================
![alt text](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Img/experiment_1_process.png)

## 1 - Train reinforcement learning models

### 1.1) - Generate datasets for SRL (random policy)

```

cd robotics-rl-srl
# Task_1: Target reaching (TR) 
python -m environments.dataset_generator --num-cpu 6 --name Omnibot_random_simple --env OmnirobotEnv-v0 --simple-continual --num-episode 250 -f
# Task_2: Target circling (TC)
python -m environments.dataset_generator --num-cpu 6 --name Omnibot_circular --env OmnirobotEnv-v0 --circular-continual --num-episode 250 -f

```
### 1.2) Train SRL

```

cd srl_zoo
# Task_1: Target reaching (TR) 
python train.py --data-folder data/Omnibot_random_simple  -bs 32 --epochs 20 --state-dim 200 --losses autoencoder:1:198 reward:1:-1 inverse:1:2 --figdir logs/figures_srl_split_task_1/ -lr 0.0005
# Task_2: Target circling (TC)
python train.py --data-folder data/Omnibot_circular  -bs 32 --epochs 20 --state-dim 200  --losses autoencoder inverse --figdir logs/figures_ae_inverse_task_2/ -lr 0.0005

```
For the  "--losses", you can refer to its choices in srl_zoo/train.py.

### 1.3) Train policy

```

cd ..
# Task_1: Target reaching (TR)  
python -m rl_baselines.train --algo ppo2 --srl-model arg.srl_model --srl-model-path srl_zoo/*path2srl_model* --num-timesteps 5000000 --env OmnirobotEnv-v0 --log-dir logs/simple/  --num-cpu 6 --simple-continual 
# Task_2: Target circling (TC)
python -m rl_baselines.train --algo ppo2 --srl-model arg.srl_model --srl-model-path srl_zoo/*path2srl_model*--num-timesteps 5000000 --env OmnirobotEnv-v0 --log-dir logs/circular/  --num-cpu 6 --circular-continual

```
For "--srl-model-path", it must be finished by "srl_model.pth".
For "--srl-model", have a look in "robotics-rl-srl/state_representation/registry.py" and add more model as you want with this form: "*name_of_model*": (SRLType.SRL, None)
## 2 - Train generative model

### 2.1) Generate on-policy datasets
```

# Task_1: Target reaching (TR) 
python -m environments.dataset_generator --env OmnirobotEnv-v0 --num-episode 1000 --run-policy custom --log-custom-policy logs/*path2policy* --save-path srl_zoo/data/ --name reaching_on_policy/ -sc --short-episodes --num-cpu 6
# Task_2: Target circling (TC)
python -m environments.dataset_generator --env OmnirobotEnv-v0 --num-episode 1000 --run-policy custom --log-custom-policy logs/*path2policy* --save-path srl_zoo/data/ --name circular_on_policy/ -cc --short-episodes --num-cpu 6

```
### 2.2) train generative model

```

cd srl_zoo
# Task_1: Target reaching (TR)
python train.py --data-folder data/reaching_on_policy  -bs 32 --epochs 20 --state-dim 200 --losses  cvae --figdir logs/figures_cvae_reaching_on_policy  --gpu-num 0 -lr 0.005
# Task_2: Target circling (TC)
python train.py --data-folder data/circular_on_policy  -bs 32 --epochs 20 --state-dim 200 --losses  cvae --figdir logs/figures_cvae_circular_on_policy  --gpu-num 0 -lr 0.005

```
There are some generative model we can use: cvae, cgan, cgan_new, gan,vae.
## 3 - Train distillation 
### 3.1) Generate dataset from generative model

```
cd ..
# Task_1: Target reaching (TR)
python -m environments.dataset_generator_from_sampling --log-custom-policy logs/*path2policyTask1* --log-generative-model srl_zoo/logs/*path2generative_model* --name generative_reaching_on_policy/ --task sc
# Task_2: Target circling (TC)
python -m environments.dataset_generator_from_sampling --log-custom-policy logs/*path2policyTask2* --log-generative-model srl_zoo/logs/*path2generative_model* --name generative_circular_on_policy/ --task cc

```
The results will be in "robotics-rl-srl/data".
### 3.2) Merge the 2 datasets 
(/ ! \ it removes the generated datasets "generative_reaching_on_policy" and "generative_circular_on_policy" from their directory and replaced by the merged dataset )

```

python -m environments.dataset_merger --merge data/generative_circular_on_policy/ data/generative_reaching_on_policy/ data/merge_generative_CC_SC

```
### 3.3) Train distillation 

```

python -m rl_baselines.train --algo distillation --srl-model raw_pixels --env OmnirobotEnv-v0 --log-dir logs/generative_CL_SC_CC/ --teacher-data-folder merge_generative_CC_SC/  -cc --epochs-distillation 20

```
## 4 - Evaluation of distilled model on each task


```

# Evaluation on task_1: Target reaching (TR)
python -m replay.enjoy_baselines --log-dir logs/*path2ditilled_policy* --num-timesteps 10000 --render --action-proba -sc
# Evaluation on task_2: Target circling (TC)
python -m replay.enjoy_baselines --log-dir logs/*path2ditilled_policy* --num-timesteps 10000 --render --action-proba -cc

```

#  EXPERIMENT 2:
Representation of the process in Experiment 2
=============================================
![alt text](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Img/experiment_2_process.png)

## 1 - Train reinforcement learning models 

### 1.1) - Generate datasets for SRL (random policy)

```

cd robotics-rl-srl
# Task_1: Target reaching (TR) 
python -m environments.dataset_generator --num-cpu 6 --name Omnibot_random_simple --env OmnirobotEnv-v0 --simple-continual --num-episode 250 -f
# Task_2: Target circling (TC)
python -m environments.dataset_generator --num-cpu 6 --name Omnibot_circular --env OmnirobotEnv-v0 --circular-continual --num-episode 250 -f

```
### 1.2) Train SRL

```

cd srl_zoo
# Task_1: Target reaching (TR) 
python train.py --data-folder data/Omnibot_random_simple  -bs 32 --epochs 20 --state-dim 200 --losses autoencoder:1:198 reward:1:-1 inverse:1:2 --figdir logs/figures_srl_split_task_1/ -lr 0.0005
# Task_2: Target circling (TC)
python train.py --data-folder data/Omnibot_circular  -bs 32 --epochs 20 --state-dim 200  --losses autoencoder inverse --figdir logs/figures_ae_inverse_task_2/ -lr 0.0005

```
For the  "--losses", you can refer to its choices in srl_zoo/train.py.

### 1.3) Train policy

```

cd ..
# Task_1: Target reaching (TR)  
python -m rl_baselines.train --algo ppo2 --srl-model arg.srl_model --srl-model-path srl_zoo/*path2srl_model* --num-timesteps 5000000 --env OmnirobotEnv-v0 --log-dir logs/simple/  --num-cpu 6 --simple-continual 
# Task_2: Target circling (TC)
python -m rl_baselines.train --algo ppo2 --srl-model arg.srl_model --srl-model-path srl_zoo/*path2srl_model*--num-timesteps 5000000 --env OmnirobotEnv-v0 --log-dir logs/circular/  --num-cpu 6 --circular-continual

```
For "--srl-model-path", it must be finished by "srl_model.pth".
For "--srl-model", have a look in "robotics-rl-srl/state_representation/registry.py" and add more model as you want with this form: "*name_of_model*": (SRLType.SRL, None)


## 2 - Train generative model

### 2.1) Generate random-policy dataset
```

# Task_1: Target reaching (TR) 
python -m environments.dataset_generator --num-cpu 6 --name Omnibot_random_simple_init_with_real --env OmnirobotEnv-v0 --simple-continual --num-episode 250 -f --init-with-real
# Task_2: Target circling (TC)
python -m environments.dataset_generator --num-cpu 6 --name Omnibot_circular_init_with_real --env OmnirobotEnv-v0 --circular-continual --num-episode 250 -f --init-with-real


```
### 2.2) train generative model

```

cd srl_zoo
# Task_1: Target reaching (TR)
python train.py --data-folder data/Omnibot_random_simple_init_with_real  -bs 128 --epochs 20 --state-dim 200 --losses  cvae_new --figdir logs/figures_cvae_new_TR  --gpu-num 0 -lr 0.005 --model-type custom_cnn_2
# Task_2: Target circling (TC)
python train.py --data-folder data/Omnibot_circular_init_with_real  -bs 128 --epochs 20 --state-dim 200 --losses  cvae_new --figdir logs/figures_cvae_new_TC --gpu-num 0 -lr 0.005 --model-type custom_cnn_2

```
We use only "cvae_new" with model_type = "custom_cnn_2" for this experiment.

#### Generated images :


![alt text](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Img/grid_walker_0.28_TR_.png)

Oberservations in a episod enerated by CVAE_Task1 with a fixed
value of latent variable z.The sampled target is at position (0.50, 0.49) and the
robot’s positions are sampled with grid walker of step 0.28. With this step, there
are 25 observations in an episode.

![alt text](https://github.com/bunthet01/robotics-rl-srl/blob/master/rl_baselines/supervised_rl/Documents/Img/grid_walker_0.28_TC_.png)
Oberservations in a episod enerated by CVAE_Task2 with a fixed
value of laten variable z. The sampled target is placed at the center of the arena
and the robot’s positions are sampledwith grid walker of step 0.28. With this
step, there are 25 observations in an episode.


## 3 - Train distillation 
### 3.1) Generate dataset from generative model

```
cd ..
# Task_1: Target reaching (TR)
python -m environments.dataset_generator_from_sampling --log-custom-policy logs/*path2policyTask1* --log-generative-model srl_zoo/logs/*path2generative_model* --name generative_TR_grid_walker/ --task sc --grid-walker
# Task_2: Target circling (TC)
python -m environments.dataset_generator_from_sampling --log-custom-policy logs/*path2policyTask2* --log-generative-model srl_zoo/logs/*path2generative_model* --name generative_TC_grid_walker/ --task cc --grid-walker

```
The results will be in "robotics-rl-srl/data".
### 3.2) Merge the 2 datasets 
(/ ! \ it removes the generated datasets "generative_TR_grid_walker" and "generative_TC_grid_walker" from their directory and replaced by the merged dataset )

```

python -m environments.dataset_merger --merge data/generative_TC_grid_walker/ data/generative_TR_grid_walker/ data/merge_generative_CC_SC_grid_walker

```
### 3.3) Train distillation 

```

python -m rl_baselines.train --algo distillation --srl-model raw_pixels --env OmnirobotEnv-v0 --log-dir logs/generative_CL_SC_CC_grid_walker/ --teacher-data-folder merge_generative_CC_SC_grid_walker/  -cc --epochs-distillation 20

```
## 4 - Evaluation of distilled model on each task


```

# Evaluation on task_1: Target reaching (TR)
python -m replay.enjoy_baselines --log-dir logs/*path2ditilled_policy* --num-timesteps 10000 --render --action-proba -sc
# Evaluation on task_2: Target circling (TC)
python -m replay.enjoy_baselines --log-dir logs/*path2ditilled_policy* --num-timesteps 10000 --render --action-proba -cc

```


